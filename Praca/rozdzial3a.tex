\chapter{Aplikacja do rozpoznawania gestów}
Projekt zakładał utworzenie aplikacji do rozpoznawania gestów w czasie rzeczywistym. Aplikacja została zaimplementowana za pomocą środowiska Visual Studio 2015 tworząc projekt WPF. Jak opisano w podrozdziale \ref{sec: WPF}, WPF to silnik graficzny do tworzenia aplikacji okienkowych, gdzie odpowiednie graficzne elementy oraz widoki są konstruowane za pomocą języka znaczników XAML. Język C\# posłużył do implementacji \textit{back-end'u} aplikacji, wykorzystano również elementy biblioteki Accord .NET, w skład których należały m.in. funkcje do obsługi kamery oraz filtracji obrazu, algorytm SURF jak również klasyfikator SVM. Dla części widocznej ze strony użytkownika zastosowano zestaw narzędzi o nazwie \textit{MahApps.Metro}, który nadpisuje domyślny styl graficznych elementów w silniku WPF.

\begin{figure}[h]
	\centering
	\includegraphics[width=16cm]{ApplicationFlowChart}
	\centering
	\caption{Schemat działania aplikacji.}
	\label{im: ApplicationFlowChart}
\end{figure}

Zasada działania aplikacji została przedstawiona na rysunku \ref{im: ApplicationFlowChart}. Próbki treningowe wraz z odpowiednimi etykietami są poddawane działaniu algorytmu SURF. Wynik operacji to zbiór punktów kluczowych dla każdego z obrazów. Następnie dokonywane jest grupowanie odpowiednich punktów w znaną z góry liczbę klastrów. Wyniki grupowania lądują w bazie danych, która jest wykorzystywana do utworzenia modelu SVM. 
Próbka testowa poddawana jest takim samym operacjom co wszystkie próbki uczące - jest ona przekształcana za pomocą algorytmu SURF oraz K-średnich w wektor o znanej liczbie klastrów. Model SVM klasyfikuje próbkę do jednej z podanych kategorii. 


W dalszej części rozdziału przedstawiono szczegółową implementację aplikacji. Pierwszy podrozdział zawiera opis wykorzystanych wzorców projektowych. W kolejnych podrozdziałach opisano każdy z widoków dostępnych w aplikacji wraz ze wzajemnymi zależnościami pomiędzy nimi.

\section{Wzorzec MVVM}
Wzorzec MVVM (skrót od ang. Model-View-ViewModel) jest jednym z wzorców architektonicznych służący do tworzenia oprogramowania. Daje możliwość pełnej separacji interfejsu użytkownika od modelu aplikacji. Twórcami wzorca są architekci firmy \textit{Microsoft} Ken Cooper oraz Ted Peters, którzy pracowali nad sposobem usprawnienia programowania sterowanego zdarzeniami. Wzorzec ten stał się jednym z komponentów nowo powstałego silnika graficznego WPF. 
W skład wzorca MVVM wchodzą następujące elementy:
\begin{itemize}
	\item Model: Reprezentuje prawdziwą zawartość aplikacji lub stanowi warstwę dostępu do danych.
	\item View (widok): jest to struktura, którą użytkownik widzi na ekranie swojego monitora.
	\item ViewModel: abstrakcyjna warstwa dla widoku udostępniająca publiczne własności oraz komendy. Za pomocą techniki \textit{data binding} widok wymienia informację z powiązanym z nim ViewModelem. ViewModel odpytuje model w celu pobrania lub zmiany danych. Zastosowanie struktury ViewModelu eliminuje istnienie bezpośredniego połączania pomiędzy widokiem a modelem.
\end{itemize}
RYSUNEK

\section{Wzorzec Mediator}
\label{sec: mediator}
W inżynierii oprogramowania mediator jest wzorcem projektowym należącym do grupy wzorców czynnościowych. Jest jednym z 23 wzorców projektowych opisanych przez tzw. \textit{"Bandę Czworga"} (z ang. \textit{Gang of Four}, GoF) w książce \textit{"Design Patterns: Elements of Reusable Object-Oriented Software"} stanowiącej jeden z kanonów tworzenia oprogramowania. Zastosowanie wzorca powoduje, że komunikacja pomiędzy obiektami jest zawarta w obiekcie mediatora. Obiekty nie komunikują się bezpośrednio ze sobą, lecz wykorzystują mediatora do pośredniej komunikacji, który koordynuje wzajemną interakcję.  Dzięki temu następuje redukcja wzajemnych zależności pomiędzy komunikującymi się obiektami. Klasa klienta może wykorzystać mediatora w celu wysłania wiadomości do innych klientów oraz może otrzymać powiadomienie zwrotne poprzez zdarzenie pochodzące od klasy mediatora. Obiekty sa w ten sposób luźno powiązane pomiędzy sobą, dzięki temu komunikacja jest łatwa w implementacji. Testowanie oraz ponowne wykorzystanie kodu jest możliwe, ponieważ obiekty muszą jedynie odnosić się do mediatora oraz nie posiadają żadnej informacji o innych elementach.

RYSUNEK





\section{Architektura aplikacji}
Architektura aplikacji została przedstawiona na rysunku \ref{im: AppArchitectureWithHsl}. Składa się ona z jednego modelu, trzech widoków oraz z powiązanymi z nimi modelami widoków. W skład modelu wchodzi klasa \textit{Camera} posiadająca flagę informującą o stanie urządzenia (czy jest włączone) oraz właściwości opisujące nazwę kamery oraz bieżącą ramkę. Model ten jest powiązany z dwoma modelami widoków: z oknem głównym oraz z oknem służącym do filtrowania obrazu w przestrzeni barw HSL. Model kamery informuje widoki modelu o zmianie swoich właściwości za pomocą zdarzenia. Możliwość zmian kamery lub jej właściwości jest możliwe jedynie za pomocą głównego okna aplikacji. Kolejną częścią modelu jest klasa powiązana z gestem. Posiada ona właściwości opisujące nazwę gestu, etykietę oraz zawiera listę wektorów opisujących każdy obraz wchodzący w skład gestu. Model ten jest powiązany z widokiem modelu dla bazy danych, jak również 

Wymiana informacji pomiędzy modelami widoków zachodzi z wykorzystaniem opisanego w sekcji \ref{sec: mediator} mediatora. Dzięki niemu elementy aplikacji są ze sobą luźno powiązane i ewentualna zmiana w architekturze aplikacji jest łatwo wykonywalna. 
\begin{figure}[h]
	\centering
	\includegraphics[width=16cm]{AppArchitectureWithHsl}
	\centering
	\caption{Architektura aplikacji.}
	\label{im: AppArchitectureWithHsl}
\end{figure}

Powiązanie pomiędzy modelem widoku a widokiem wykorzystuje wzorzec projektowy o nazwie \textit{View Model Locator}. Wykorzystanie tego wzorca stanowi standardowy, spójny oraz deklaratywny sposób powiązania widoku z modelem widoku  w podejściu \textit{view first}. Zasada łączenia widoku modelu z widokiem można opisać następujacymi krokami:
\begin{enumerate}
	\item \textit{View Model Locator} sprawdza jaki widok ma zostać stworzony.
	\item Wzorzec identyfikuje widok modelu dla  tworzonego widoku.
	\item \textit{View Model Locator} konstruuje widok modelu.
	\item Wysłanie danych ustawionych w widoku do \textit{View Model'u}.
\end{enumerate}

Baza danych jest w postaci pliku XML (skrót on ang. \textit{Extensible Markup Language}). Zawiera ona informacje o wszystkich gestach dodanych do bazy. Każdy węzeł występujący w pliku posiada dwa atrybuty: nazwę gestu oraz numer etykiety. Wewnątrz każdego węzła znajdują się rekordy, które są mapowane na  poszczególne obrazy w obrębie gestu. Pojedynczy rekord posiada dwa atrybuty: nazwę pliku oraz wektor cech opisujący dany gest.

\subsection{Główny widok aplikacji}
Na rysunku RYSUNEK przedstawiono widok główny aplikacji do rozpoznawania gestów. W jego skład wchodzą następujące elementy:
\begin{itemize}
	\item Bieżąca ramka kamery. Jest to element stanowiący największą część głównego okna. Odświeżanie obrazu zachodzi po każdym zdarzeniu wynikającym z pojawienia się nowej ramki w modelu kamery.
	\item Lista rozwijana, której elementy reprezentują urządzenia wideo podłączone do komputera. Zmiana kamery możliwa jedynie w przypadku, gdy obecnie wybrana kamera jest rozłączona.
	\item Element umożliwiający zmianę rozdzielczości dla wybranej kamery. Zmiana jest możliwa jedynie w przypadku, gdy wybrana kamera jest nie rejestruje strumienia wideo.
	\item Przyciski \textit{CONNECT} oraz \textit{DISCONNECT}. Odpowiadają za start oraz zatrzymanie wybranej kamery. W przypadku, gdy kamera jest uruchomiona przycisk powiązany z startem kamery zostaje wyszarzany. Analogiczne zachowanie występuje dla przycisku \textit{DISCONNECT}.
	\item Przycisk \textit{TAKE SNAPSHOT}. Naciśnięcie przycisku powoduje pobranie prostokąta otaczającego stanowiącego część bieżącej ramki ze strumienia wideo. Prostokąt zostaje wysłany do widoku modelu powiązanego z bazą danych, gdzie zostaje sklasyfikowany oraz przypisany do istniejącego gestu. Działanie jest możliwe wyłącznie wtedy, gdy wybrana kamera jest włączona oraz gdy w bazie danych jest więcej niż jeden gest. 
	\item Przycisk \textit{SET HSL FILTER}. Otwarcie okna, w którym możliwa jest edycja składowych filtra przestrzeni barw HSL. Szczegóły w podrozdziale PODROZDZIAŁ
	\item Przycisk \textit{OPEN DATABASE}. Naciśnięcie przycisku otwiera widok bazy gestów. Szczegółowe informacje o zachowaniu bazy są dostępne w podrozdziale ROZDZIAŁ
\end{itemize}


\subsection{Okno filtrowania}
Okno filtrowania zostało przedstawione na rysunku RYSUNEK. Podobnie jak w  przypadku głównego okna, najważniejszy element stanowi obraz z bieżącą ramką kamery. Powyżej ramki znajdują się elementy związane z filtrowaniem przestrzeni barw HSL. Istnieje możliwość zmian zakresu wartości poszczególnej składowej, a wyniki tych operacji sa widoczne na ramce kamery. Wyliczanie obrazu z nałożonym filtrem następuje po każdym zdarzeniu powiązanym z pojawieniem się nowej ramki w modelu kamery.
Obraz jest tworzony w taki sposób, że jeżeli dany piksel z oryginalnego obrazu spełnia kryteria nałożonego filtru, to taki piksel zostaje wyświetlony. W przeciwnym wypadku przypisuje mu się wartość koloru czarnego. Dodatkowo na obszarze obrazu zastosowano prostokąt otaczający, który obejmuje największy element znaleziony po etapie filtrowania. Następnie prostokąt zostaje wysłany do widoku głównego i tam wyświetlony na obrazie oryginalnym. 
\subsection{Okno bazy danych}

Na rysunku RYSUNEK przedstawiono okno bazy danych dla aplikacji rozpoznawania gestów. W obrębie okna można wyodrębnić dwa główne elementy: widok zakładek oraz panel umożliwiający konfigurację bazy danych. 

Wszystkie gesty dostępne w bazie są widoczne w widoku zakładek. Nazwy gestów stanowią nagłówki, które użytkownik może przełączać w celu zaciągnięcia informacji o danym geście. Pojedyncza zakładka zawiera listę elementów, a każdy rekord składa się ze zdjęcia gestu, nazwy pliku oraz deskryptora opisującego pojedynczy gest.

Po prawej stronie okna umieszczono panel służący do konfiguracji bazy gestów. Pierwszy element stanowi przycisk umożliwiający dodanie nowego gestu. Po jego kliknięciu zostaje otworzony prosty widok, w którym użytkownik może wczytać pliki zawierające gesty. W przypadku, gdy nazwa wczytywanego gestu istnieje w bazie, obrazy zostaną dodane właśnie do tego elementu bazy danych. 

Pierwsza grupa elementów umieszczona poniżej przycisku do dodawania gestów powiązana jest z ekstrakcją cech. W jej skład wchodzi edytowalne pole numeryczne, w którym użytkownik podaje długość wektora cech. Drugi element to przycisk służący do wyliczania deskryptora. Po jego kliknięciu wszystkie obrazy z bazy danych są poddawane działaniu algorytmu SURF, który wylicza ekstrema dla każdego z nich. Aby wszystkie obrazy były opisane przez deskryptor o takiej samej długości, konieczne jest zastosowanie algorytmu k-średnich. Algorytm ten tworzy klastry, których liczba jest podana przez użytkownika.

Kolejna grupa właściwości jest powiązana z ustawieniami klasyfikatora. W aplikacji wykorzystano dostępny w bibliotece Accord.NET klasyfikator rozwiązujący problem klasyfikacji dla wielu kategorii. Użytkownik ma możliwość wyboru pomiędzy metodami tzw. \textit{kernel tricku'u} opisanego w podrozdziale \ref{sec: SVM}. W aplikacji zaimplementowano wielomianową niejednorodną funkcję jądra oraz metodę opartą o jądro RBF. Dla metody wielomianowej użytkownik ma możliwość podania stopnia wielomianu oraz stałej, a dla metody opartej o rozkład Gaussa jest możliwa zmiana parametru $\sigma$. Dla obu metod istnieje możliwość konfiguracja dwóch parametrów: parametru dla funkcji kary, która zostaje nałożona w przypadku złej klasyfikacji próbek treningowych oraz tolerancji, której wartość stanowi kryterium stopu podczas uczenia klasyfikatora SVM. 

Kolejne dwa przyciski są powiązane z uczeniem modelu SVM oraz do klasyfikacji próbek treningowych.  Dla każdego obrazu gestu z widoku zakładek po procesie klasyfikacji zostaje przypisany kolor tła: zielony w przypadku poprawnego rezultatu, czerwony dla złego. Dodatkowo, gdy oba procesy zostaną wykonane, na ekranie wyświetlane są informacje dotyczące czasu trwania klasyfikacji oraz jej dokładności. Są one wykorzystane podczas porównywania działania klasyfikatora dla różnych wartości parametrów.

Ostatni przycisk służy do otwarcia widoku służącego do testowania zbioru próbek. Użytkownik podaje zbiór obrazów oraz specyfikuje nazwę gestu, dla którego zostaje przeprowadzona klasyfikacja. Dodatkowo w widoku dostępny jest przycisk \textit{TEST}, po którym następuje klasyfikacja każdego z obrazu. Możliwość testowania istnieje jedynie w przypadku, gdy przynajmniej jeden obraz znajduje się w widoku. Kolor tła dla każdej próbki wskazuje na poprawność klasyfikatora. Pomimo dobrej klasyfikacji obrazu, nie zostanie on dodany do istniejącego gestu. Konieczne jest użycie przycisku \textit{ADD NEW GESTURE}.



























